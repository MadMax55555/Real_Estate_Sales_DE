{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1401f837-bb59-4eeb-bb6c-05e1c8f84f5f",
   "metadata": {},
   "source": [
    "\n",
    "# Real Estate Sales ETL Pipeline\n",
    "\n",
    "## Introduction\n",
    "This notebook outlines the ETL (Extract, Transform, Load) process for real estate sales data obtained from the [Office of Policy and Management](https://data.ct.gov/Housing-and-Development/Real-Estate-Sales-2001-2022-GL/5mzw-sjtu/about_data). The dataset contains records of real estate sales, including property type, sales price, and assessment values. The primary goal of this project is to clean, transform, and load this data into a PostgreSQL data warehouse to facilitate efficient analysis and reporting on property trends over the years.\n",
    "\n",
    "By implementing this ETL pipeline, the project demonstrates the ability to clean, structure, and load complex real estate data, making it suitable for analysis. This is crucial for understanding market trends and supporting data-driven decision-making in the real estate industry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fd8d8b-0e71-4fec-9e0c-f46c4c34d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f5742-870f-4981-a2c5-52d23345e4b6",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b211d-48e2-49c7-9c66-ddd27b2f819b",
   "metadata": {},
   "source": [
    "## Load Real Estate Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440391dc-af44-4a3a-aa63-6b6f4aa2ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real estate sales data is loaded from a local CSV file. \n",
    "#The 'low_memory=False' option helps ensure that pandas loads the data correctly, especially when dealing with large datasets.\n",
    "real_estate_sales = pd.read_csv('data/Real_Estate_Sales_2001-2022_GL.csv', low_memory=False)\n",
    "\n",
    "# Optionally, you can load the latest data directly from the source if needed (uncomment the following line)\n",
    "# real_estate_sales = pd.read_csv(\"https://data.ct.gov/resource/5mzw-sjtu.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a8d153-ad17-428f-98f1-a80918fca7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Non-Usable Codes Data\n",
    "# This dataset contains special codes that explain why a sale might not be used for property value assessments.\n",
    "non_usable_codes = pd.read_csv('data/Non_Usable_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7d81a8-6bc1-47f2-ad07-d5204a435017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial Number</th>\n",
       "      <th>List Year</th>\n",
       "      <th>Date Recorded</th>\n",
       "      <th>Town</th>\n",
       "      <th>Address</th>\n",
       "      <th>Assessed Value</th>\n",
       "      <th>Sale Amount</th>\n",
       "      <th>Sales Ratio</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Residential Type</th>\n",
       "      <th>Non Use Code</th>\n",
       "      <th>Assessor Remarks</th>\n",
       "      <th>OPM remarks</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020177</td>\n",
       "      <td>2020</td>\n",
       "      <td>04/14/2021</td>\n",
       "      <td>Ansonia</td>\n",
       "      <td>323 BEAVER ST</td>\n",
       "      <td>133000.0</td>\n",
       "      <td>248400.0</td>\n",
       "      <td>0.5354</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single Family</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-73.06822 41.35014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020225</td>\n",
       "      <td>2020</td>\n",
       "      <td>05/26/2021</td>\n",
       "      <td>Ansonia</td>\n",
       "      <td>152 JACKSON ST</td>\n",
       "      <td>110500.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>0.4606</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Three Family</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020348</td>\n",
       "      <td>2020</td>\n",
       "      <td>09/13/2021</td>\n",
       "      <td>Ansonia</td>\n",
       "      <td>230 WAKELEE AVE</td>\n",
       "      <td>150500.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020090</td>\n",
       "      <td>2020</td>\n",
       "      <td>12/14/2020</td>\n",
       "      <td>Ansonia</td>\n",
       "      <td>57 PLATT ST</td>\n",
       "      <td>127400.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>0.6291</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Two Family</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200500</td>\n",
       "      <td>2020</td>\n",
       "      <td>09/07/2021</td>\n",
       "      <td>Avon</td>\n",
       "      <td>245 NEW ROAD</td>\n",
       "      <td>217640.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Single Family</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial Number  List Year Date Recorded     Town          Address  \\\n",
       "0        2020177       2020    04/14/2021  Ansonia    323 BEAVER ST   \n",
       "1        2020225       2020    05/26/2021  Ansonia   152 JACKSON ST   \n",
       "2        2020348       2020    09/13/2021  Ansonia  230 WAKELEE AVE   \n",
       "3        2020090       2020    12/14/2020  Ansonia      57 PLATT ST   \n",
       "4         200500       2020    09/07/2021     Avon     245 NEW ROAD   \n",
       "\n",
       "   Assessed Value  Sale Amount  Sales Ratio Property Type Residential Type  \\\n",
       "0        133000.0     248400.0       0.5354   Residential    Single Family   \n",
       "1        110500.0     239900.0       0.4606   Residential     Three Family   \n",
       "2        150500.0     325000.0       0.4630    Commercial              NaN   \n",
       "3        127400.0     202500.0       0.6291   Residential       Two Family   \n",
       "4        217640.0     400000.0       0.5441   Residential    Single Family   \n",
       "\n",
       "  Non Use Code Assessor Remarks OPM remarks                    Location  \n",
       "0          NaN              NaN         NaN  POINT (-73.06822 41.35014)  \n",
       "1          NaN              NaN         NaN                         NaN  \n",
       "2          NaN              NaN         NaN                         NaN  \n",
       "3          NaN              NaN         NaN                         NaN  \n",
       "4          NaN              NaN         NaN                         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_estate_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82df12b1-4736-4492-b9b2-126575ea4dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Not special</td>\n",
       "      <td>Nothing special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>Sale between members of the same family.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LOVE AND AFFECTION</td>\n",
       "      <td>Sale in which 'Love and affection' are part of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>INTER CORPORATION</td>\n",
       "      <td>Sales between a corporation and stockholder, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CORRECTING DEED</td>\n",
       "      <td>Transfers of convenience; for example, sales f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id               Title                                         Description\n",
       "0    0         Not special                                    Nothing special\n",
       "1    1              FAMILY           Sale between members of the same family.\n",
       "2    2  LOVE AND AFFECTION  Sale in which 'Love and affection' are part of...\n",
       "3    3   INTER CORPORATION  Sales between a corporation and stockholder, s...\n",
       "4    4     CORRECTING DEED  Transfers of convenience; for example, sales f..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_usable_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33317c55-f904-4b7d-ba5e-dc82a5b285e6",
   "metadata": {},
   "source": [
    "### Data Description\r\n",
    "The dataset contains detailed records of real estate transactions. Below is a summary of each column:\r\n",
    "\r\n",
    "#### Real Estate Sales Data:\r\n",
    "| **Column Name**        | **Description**                                                                                     | **Data Type**              | **Example Value**          |\r\n",
    "|------------------------|-----------------------------------------------------------------------------------------------------|----------------------------|----------------------------|\r\n",
    "| **Serial Number**       | A unique identifier for each property sale record.                                                 | Integer                    | 2020177                    |\r\n",
    "| **List Year**           | The year the property was listed for sale.                                                         | Integer                    | 2020                       |\r\n",
    "| **Date Recorded**       | The date when the sale was recorded in the local jurisdiction.                                      | Date (Floating Timestamp)  | 04/14/2021                 |\r\n",
    "| **Town**                | The name of the town where the property is located.                                                | Text                       | Ansonia                    |\r\n",
    "| **Address**             | The full address of the property sold.                                                              | Text                       | 323 BEAVER ST              |\r\n",
    "| **Assessed Value**      | The value of the property used for local tax assessment.                                            | Float (Number)             | 133000.0                   |\r\n",
    "| **Sale Amount**         | The final sale price for the property.                                                              | Float (Number)             | 248400.0                   |\r\n",
    "| **Sales Ratio**         | The ratio of the sale price to the assessed value, providing insight into property valuation.       | Float (Number)             | 0.5354                     |\r\n",
    "| **Property Type**       | The type of property sold (e.g., Residential, Commercial, Industrial, etc.).                        | Text                       | Residential                |\r\n",
    "| **Residential Type**    | Indicates whether the property is a single-family or multi-family residential property.            | Text                       | Single Family              |\r\n",
    "| **Non Use Code**        | A code indicating that the sale price is not reliable for determining property value.               | Text                       | NaN                        |\r\n",
    "| **Assessor Remarks**    | Remarks or comments made by the property assessor regarding the property.                           | Text                       | NaN                        |\r\n",
    "| **OPM Remarks**         | Comments from the Office of Policy and Management regarding the property.                          | Text                       | NaN                        |\r\n",
    "| **Location**            | Geographical coordinates (latitude and longitude) representing the property's location.            | Point                      | POINT (-73.06822 41.35014) |\r\n",
    "\r\n",
    "#### Non-Usable Codes Data:\r\n",
    "This dataset provides a mapping of codes that describe special cases when property sale data is not reliable for analysis. These cases include sales such as transfers within the same family or sales made for convenience reasons.\r\n",
    "\r\n",
    "These codes will be used to filter or tag specific sale transactions for analysis later.\r\n",
    "\r\n",
    "| **Id** | **Title**              | **Description**                                                                                     |\r\n",
    "|--------|------------------------|-----------------------------------------------------------------------------------------------------|\r\n",
    "| 0      | Not special            | Nothing special.                                                                                   |\r\n",
    "| 1      | FAMILY                 | Sale between members of the same family.                                                           |\r\n",
    "| 2      | LOVE AND AFFECTION      | Sale in which 'Love and affection' are part of the deal.                                           |\r\n",
    "| 3      | INTER CORPORATION      | Sales between a corporation and stockholder.                                                       |\r\n",
    "| 4      | CORRECTING DEED        | Transfers of convenience; for example, sales for correction of deeds.                             |\r\n",
    "\r\n",
    "### Next Steps:\r\n",
    "At this stage, the data has been ingested into pandas DataFrames, and we've inspected a sample of the data. The next steps will involve:\r\n",
    "- **Data Cleaning**: Removing any irrelevant or missing data.\r\n",
    "- **Data Transformation**: Converting data types, handling missing values, and creating derived columns if necessary.\r\n",
    "- **Data Loading**: Loading the cleaned and transformed data into a PostgreSQL data warehouse for analysis.\r\n",
    "22 41.35014) |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268bd42-1d11-477f-a2a2-29c9a47f5564",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5ef1eb-3458-4c35-add3-e3e002bbd14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Serial Number</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>List Year</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Recorded</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Town</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Address</th>\n",
       "      <td>51</td>\n",
       "      <td>0.004646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assessed Value</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sale Amount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales Ratio</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property Type</th>\n",
       "      <td>382446</td>\n",
       "      <td>34.842921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residential Type</th>\n",
       "      <td>398389</td>\n",
       "      <td>36.295415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Use Code</th>\n",
       "      <td>784178</td>\n",
       "      <td>71.442901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assessor Remarks</th>\n",
       "      <td>926401</td>\n",
       "      <td>84.400194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPM remarks</th>\n",
       "      <td>1084598</td>\n",
       "      <td>98.812805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>799518</td>\n",
       "      <td>72.840459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Missing Values  Percentage\n",
       "Serial Number                  0    0.000000\n",
       "List Year                      0    0.000000\n",
       "Date Recorded                  2    0.000182\n",
       "Town                           0    0.000000\n",
       "Address                       51    0.004646\n",
       "Assessed Value                 0    0.000000\n",
       "Sale Amount                    0    0.000000\n",
       "Sales Ratio                    0    0.000000\n",
       "Property Type             382446   34.842921\n",
       "Residential Type          398389   36.295415\n",
       "Non Use Code              784178   71.442901\n",
       "Assessor Remarks          926401   84.400194\n",
       "OPM remarks              1084598   98.812805\n",
       "Location                  799518   72.840459"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values and their percentage in the dataset\n",
    "missing_values = real_estate_sales.isnull().sum()\n",
    "missing_percentage = (missing_values / len(real_estate_sales)) * 100\n",
    "\n",
    "# Creating a DataFrame to summarize the missing values and their percentage\n",
    "missing_analysis = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "\n",
    "# Displaying the missing analysis for the user to inspect\n",
    "missing_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa972a77-3fd9-43b3-8d79-1dd59d3c27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing irrelevant columns that have too many missing values or aren't useful for analysis\n",
    "real_estate_sales = real_estate_sales.drop(['Assessor Remarks', 'OPM remarks', 'Location', 'Serial Number'], axis=1)\n",
    "\n",
    "# Dropping rows where critical fields like 'Address' and 'Date Recorded' are missing\n",
    "real_estate_sales = real_estate_sales.dropna(subset=['Address', 'Date Recorded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ae389-15ec-44c0-95a9-243b2543b4c3",
   "metadata": {},
   "source": [
    "## Non usable codes cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975f6b0c-19cd-4df2-8d23-539c0e1c4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling 'Non Use Code' missing data: fill with 0 and process specific cases\n",
    "real_estate_sales['Non Use Code'] = real_estate_sales['Non Use Code'].fillna(0)\n",
    "real_estate_sales.loc[real_estate_sales['Non Use Code'] == 'Single Family', 'Non Use Code'] = 0\n",
    "real_estate_sales.loc[real_estate_sales['Non Use Code'] == 'Single Family', 'Residential Type'] = 'Single Family'\n",
    "real_estate_sales.loc[real_estate_sales['Non Use Code'] == 'Single Family', 'Property Type'] = 'Residential'\n",
    "real_estate_sales['Non Use Code'] = real_estate_sales['Non Use Code'].apply(lambda x: x.split(\" -\")[0] if isinstance(x, str) else x)\n",
    "real_estate_sales['Non Use Code'] = pd.to_numeric(real_estate_sales['Non Use Code'], errors='coerce')\n",
    "real_estate_sales.loc[real_estate_sales['Non Use Code'] > 30, 'Non Use Code'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aed0fd-8c52-48ca-9992-36a860f8e83f",
   "metadata": {},
   "source": [
    "## Property Type et Residential Type cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65efc0c7-83da-4f2e-a005-213f0d83591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the 'Property Type' column and handling 'Apartments' as a special case\n",
    "real_estate_sales.loc[:, 'Property Type'] = real_estate_sales['Property Type'].replace({\n",
    "    'Two Family': 'Residential',\n",
    "    'Single Family': 'Residential',\n",
    "    'Three Family': 'Residential',\n",
    "    'Four Family': 'Residential',\n",
    "    'Condo': 'Residential',\n",
    "})\n",
    "\n",
    "# Handling apartments as a separate residential type\n",
    "real_estate_sales.loc[real_estate_sales['Property Type'] == 'Apartments', 'Residential Type'] = 'Apartments'\n",
    "real_estate_sales.loc[real_estate_sales['Property Type'] == 'Apartments', 'Property Type'] = 'Residential'\n",
    "real_estate_sales.loc[:, 'Property Type'] = real_estate_sales['Property Type'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d797019-36f9-4e3d-a66f-b483c4ec5fc3",
   "metadata": {},
   "source": [
    "## List Year et Date Recorded cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08381f1-c66b-4408-9a9c-b567625760cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'List Year' to numeric and 'Date Recorded' to datetime format, and extracting time-based features\n",
    "real_estate_sales['List Year'] = pd.to_numeric(real_estate_sales['List Year'], errors='coerce')\n",
    "real_estate_sales['Date Recorded'] = pd.to_datetime(real_estate_sales['Date Recorded'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# Extracting day, month, year, and quarter from 'Date Recorded'\n",
    "real_estate_sales.loc[:, 'Sale Day'] = real_estate_sales['Date Recorded'].dt.day\n",
    "real_estate_sales.loc[:, 'Sale Month'] = real_estate_sales['Date Recorded'].dt.month\n",
    "real_estate_sales.loc[:, 'Sale Year'] = real_estate_sales['Date Recorded'].dt.year\n",
    "real_estate_sales.loc[:, 'Sale Quarter'] = real_estate_sales['Date Recorded'].dt.quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2dee8-160b-4446-bfd6-81f2ea59cc26",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595c36c-c7b6-4182-9ad4-62a615722fcf",
   "metadata": {},
   "source": [
    "1. **Connecting to PostgreSQL Database**:\n",
    "   - A connection to the PostgreSQL database is established using SQLAlchemy's `create_engine`. The credentials for PostgreSQL (username, password, host, port, and database name) are provided to connect successfully.\n",
    "   - After establishing the connection, a test query (`SELECT version()`) is executed to confirm the connection is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40cb0a4a-6614-4888-ab92-2d3bb8fced87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PostgreSQL 16.3, compiled by Visual C++ build 1938, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "# PostgreSQL credentials\n",
    "username = 'postgres'\n",
    "password = 'password'  # Change this with your password\n",
    "host = 'localhost'  # or your server IP\n",
    "port = '5432'       # default PostgreSQL port\n",
    "database = 're_sales_dw'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Test the connection\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT version();\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b748e0-222f-4f04-b40d-32fa2068986d",
   "metadata": {},
   "source": [
    "2. **Loading and Inserting dim_list_year Data**:\n",
    "   - The unique List Year values from the real estate dataset are extracted and loaded into a DataFrame (dim_list_year_df).\n",
    "   - The code checks if these years already exist in the dim_list_year table in the database and inserts only the new years that are not yet present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a2fef2-8522-41b3-b497-dcd9c0de05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_list_year_df = pd.DataFrame({'year': real_estate_sales['List Year'].unique()})\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Load existing years from dim_list_year table\n",
    "    existing_years = pd.read_sql('SELECT year FROM dim_list_year', conn)\n",
    "\n",
    "    # Step 2: Find new years that are not already in dim_list_year\n",
    "    new_years_df = dim_list_year_df[~dim_list_year_df['year'].isin(existing_years['year'])]\n",
    "    \n",
    "    # Step 3: Append only the new records to dim_list_year table\n",
    "    if not new_years_df.empty:\n",
    "        new_years_df.to_sql('dim_list_year', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec0ba4-4e9c-440d-912f-b6c43fc89013",
   "metadata": {},
   "source": [
    "3. **Loading and Inserting dim_location Data**:\n",
    "   - The Town and Address columns are used to create unique location records.\n",
    "   - The code checks for new locations not yet stored in the dim_location table and appends them if not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80bd867-8dab-4041-9176-7ee69cffd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dim_location_df by dropping duplicates from the original DataFrame\n",
    "dim_location_df = real_estate_sales[['Town', 'Address']].drop_duplicates()\n",
    "dim_location_df.columns = ['city', 'street_address']  # Rename columns to match your dimension table\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Retrieve existing records from dim_location table\n",
    "    existing_locations = pd.read_sql('SELECT city, street_address FROM dim_location', conn)\n",
    "\n",
    "    # Step 2: Find new locations that are not already in dim_location\n",
    "    new_locations_df = dim_location_df.merge(\n",
    "        existing_locations, \n",
    "        how='left', \n",
    "        on=['city', 'street_address'], \n",
    "        indicator=True\n",
    "    ).loc[lambda x: x['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "    # Step 3: Append only the new records to dim_location table\n",
    "    if not new_locations_df.empty:\n",
    "        new_locations_df.to_sql('dim_location', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9d548-98a8-4b78-8952-f6693a78007e",
   "metadata": {},
   "source": [
    "4. **Loading and Inserting dim_non_use_code Data**:\n",
    "   - The Non Use Code is cleaned and columns are renamed to match the target table.\n",
    "   - New non-use codes that are not yet present in the dim_non_use_code table are identified and inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164431b1-62b9-48e3-9f30-074bcb14c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match your dimension table\n",
    "non_usable_codes.columns = ['id', 'title', 'description']\n",
    "\n",
    "# Connect to the database and perform the insert operation\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Retrieve existing records from dim_non_use_code table\n",
    "    existing_codes = pd.read_sql('SELECT id, title, description FROM dim_non_use_code', conn)\n",
    "\n",
    "    # Step 2: Find new codes that are not already in dim_non_use_code\n",
    "    new_codes_df = non_usable_codes.merge(\n",
    "        existing_codes, \n",
    "        how='left', \n",
    "        on=['id', 'title', 'description'], \n",
    "        indicator=True\n",
    "    ).loc[lambda x: x['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "    # Step 3: Append only the new records to dim_non_use_code table\n",
    "    if not new_codes_df.empty:\n",
    "        new_codes_df.to_sql('dim_non_use_code', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbed468-f3d7-47a2-96d6-e4c0d623de8e",
   "metadata": {},
   "source": [
    "5. **Loading and Inserting dim_property_type Data**:\n",
    "   - The Property Type and Residential Type columns are used to create unique records in the dimension table.\n",
    "   - The code checks for new property types not yet in the dim_property_type table and inserts them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a911409-9d12-4e81-9cac-4f3ee2088bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_propriety_type_df = real_estate_sales[['Property Type', 'Residential Type']].drop_duplicates()\n",
    "dim_propriety_type_df.columns = ['property_type', 'residential_type'] \n",
    "# Connect to the database and perform the insert operation\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Retrieve existing records from dim_property_type table\n",
    "    existing_property_types = pd.read_sql('SELECT * FROM dim_property_type', conn)\n",
    "\n",
    "    # Step 2: Find new property types that are not already in dim_property_type\n",
    "    new_property_types_df = dim_propriety_type_df.merge(\n",
    "        existing_property_types, \n",
    "        how='left', \n",
    "        on=dim_propriety_type_df.columns.tolist(),  # Join on all columns\n",
    "        indicator=True\n",
    "    ).loc[lambda x: x['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "    # Step 3: Append only the new records to dim_property_type table\n",
    "    if not new_property_types_df.empty:\n",
    "        new_property_types_df[['property_type', 'residential_type']].to_sql('dim_property_type', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d657c6-db96-470b-809c-1be32057d18c",
   "metadata": {},
   "source": [
    "6. **Loading and Inserting dim_sale_date Data**:\n",
    "   - The Sale Day, Sale Month, Sale Year, and Sale Quarter columns are used to create a unique set of sale date records.\n",
    "   - New sale dates are identified and inserted into the dim_sale_date table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdbac9ef-ce9f-4430-ad45-3b03e1c83189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_sale_date_df = real_estate_sales[['Sale Day','Sale Month','Sale Year','Sale Quarter']].drop_duplicates()\n",
    "dim_sale_date_df.columns = ['day','month','year','quarter'] \n",
    "# Connect to the database and perform the insert operation\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Retrieve existing records from dim_sale_date table\n",
    "    existing_sale_dates = pd.read_sql('SELECT * FROM dim_sale_date', conn)\n",
    "\n",
    "    # Step 2: Find new sale dates that are not already in dim_sale_date\n",
    "    new_sale_dates_df = dim_sale_date_df.merge(\n",
    "        existing_sale_dates, \n",
    "        how='left', \n",
    "        on=dim_sale_date_df.columns.tolist(),  # Join on all columns\n",
    "        indicator=True\n",
    "    ).loc[lambda x: x['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "    # Step 3: Append only the new records to dim_sale_date table\n",
    "    if not new_sale_dates_df.empty:\n",
    "        new_sale_dates_df[['day','month','year','quarter']].to_sql('dim_sale_date', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fdfc1-a200-45bd-a3d1-029ecb40ded4",
   "metadata": {},
   "source": [
    "7. **Creating the Final fact_sales Table**:\n",
    "   - After loading the data into the dimension tables, the fact table is created by merging the real estate sales data with the dimension tables.\n",
    "   - Duplicate rows are removed, and only the new records are inserted into the fact_sales table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28c13e4a-b2c2-4890-8cd9-049441f3c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    dim_list_year = pd.read_sql('SELECT * FROM dim_list_year', connection)\n",
    "    dim_location = pd.read_sql('SELECT * FROM dim_location', connection)\n",
    "    dim_property_type = pd.read_sql('SELECT * FROM dim_property_type', connection)\n",
    "    dim_non_use_code = pd.read_sql('SELECT * FROM dim_non_use_code', connection)\n",
    "    dim_sale_date = pd.read_sql('SELECT * FROM dim_sale_date', connection)\n",
    "final_fact_sales = (\n",
    "    real_estate_sales.merge(\n",
    "        dim_list_year.rename(columns={'year': 'id_list_year'}),\n",
    "        left_on='List Year',\n",
    "        right_on='id_list_year',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        dim_location.rename(columns={'id': 'id_location'}),\n",
    "        left_on=['Town', 'Address'],\n",
    "        right_on=['city', 'street_address'],\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        dim_property_type.rename(columns={'id_property_type': 'id_property_type'}),\n",
    "        left_on=['Property Type', 'Residential Type'],\n",
    "        right_on=['property_type', 'residential_type'],\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        dim_non_use_code.rename(columns={'id': 'id_non_use_code'}),\n",
    "        left_on='Non Use Code',\n",
    "        right_on='id_non_use_code',\n",
    "        how='left'\n",
    "    ).merge(  # Added the missing period here\n",
    "        dim_sale_date.rename(columns={'id_sale_date': 'id_sale_date'}),\n",
    "        left_on=['Sale Day', 'Sale Month', 'Sale Year', 'Sale Quarter'],\n",
    "        right_on=['day', 'month', 'year', 'quarter'],\n",
    "        how='left'\n",
    "    )[\n",
    "        [\n",
    "            'id_list_year', 'id_location', 'id_property_type', 'id_non_use_code', 'id_sale_date',\n",
    "            'Assessed Value', 'Sale Amount', 'Sales Ratio'\n",
    "        ]\n",
    "    ].rename(columns={\n",
    "        'Assessed Value': 'assessed_value',\n",
    "        'Sale Amount': 'sale_amount',\n",
    "        'Sales Ratio': 'sales_ratio'\n",
    "    })\n",
    ")\n",
    "duplicate_columns = ['id_list_year', 'id_location', 'id_property_type', 'id_non_use_code', 'id_sale_date']\n",
    "\n",
    "# Drop duplicates while keeping the first occurrence\n",
    "final_fact_sales_unique = final_fact_sales.drop_duplicates(subset=duplicate_columns)\n",
    "# Specify the columns that have the unique constraint in fact_sales\n",
    "unique_columns = ['id_list_year', 'id_location', 'id_property_type', 'id_non_use_code', 'id_sale_date']\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Step 1: Retrieve existing unique values from the fact_sales table\n",
    "    existing_fact_sales = pd.read_sql(f'SELECT {\", \".join(unique_columns)} FROM fact_sales', conn)\n",
    "\n",
    "    # Step 2: Identify new records that are not in fact_sales\n",
    "    new_fact_sales_df = final_fact_sales_unique.merge(\n",
    "        existing_fact_sales,\n",
    "        how='left',\n",
    "        on=unique_columns,\n",
    "        indicator=True\n",
    "    ).loc[lambda x: x['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "    # Step 3: Insert only the new records\n",
    "    if not new_fact_sales_df.empty:\n",
    "        new_fact_sales_df.to_sql('fact_sales', conn, if_exists='append', index=False)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ecf91-96df-4d05-9bd9-4e59f4c29803",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we successfully implemented a data ingestion pipeline to load data from the real estate sales dataset into a PostgreSQL database. The process involved:\n",
    "\n",
    "1. **Connecting to the PostgreSQL database**: We established a connection to the database using SQLAlchemy and verified the connection with a simple query.\n",
    "2. **Loading data into dimension tables**: We processed and inserted unique records into the `dim_list_year`, `dim_location`, `dim_non_use_code`, `dim_property_type`, and `dim_sale_date` tables, ensuring no duplicates were inserted.\n",
    "3. **Creating the fact table**: By merging the original dataset with the dimension tables, we constructed the `fact_sales` table, removing duplicates and inserting only new records.\n",
    "\n",
    "This approach ensures data consistency and avoids redundancy, while optimizing the ETL process. The steps provided can be easily adapted for future datasets, enabling streamlined data ingestion and integration into the data warehouse.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
